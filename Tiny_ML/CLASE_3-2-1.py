#link a colab
#https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-3-7-FashionMNISTConvolutionsVisualizations.ipynb

#Lets retrain our convolutional model for the Fashion-MNIST dataset and then visualize the filters and pooling.

import tensorflow as tf
mnist = tf.keras.datasets.fashion_mnist
(training_images, training_labels), (val_images, val_labels) = mnist.load_data()
training_images=training_images.reshape(60000, 28, 28, 1)
training_images=training_images / 255.0
val_images=val_images.reshape(10000, 28, 28, 1)
val_images=val_images/255.0
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),
  #aplico 64 filtros de 3x3
  #osea el output es de 64 chanels de 26x26 porque no ocupe ningun padding por lo que me como una linea de pixeles a cada lado 

  tf.keras.layers.MaxPooling2D(2, 2),
  #disminuyo la resolucion a la mitad en cada dimencion => 13x13 pero mantengo los chanels

  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
   #aplico 64 filtros de 3x3
  #osea el output es de 64 chanels de 13x13 porque no ocupe ningun padding por lo que me como una linea de pixeles a cada lado 
  #ojo, a cada canal llegan 64 imagenes las cuales se comprimen y a esta final comprimida se le aplican otros 64 filtros
  #ademas como no hay pading, vuelvo a bajar la resolucion en un pixel a cada lado

  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Flatten(),
  #una vez que ya tengo mi imagen final trabajada, aplico el flaten y la ingreso a la red neuronal para hacer el analisis
  tf.keras.layers.Dense(20, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary() #esta linea me imprime todos los chanels

model.fit(training_images, training_labels, validation_data=(val_images, val_labels), epochs=20)

#Visualizing the Convolutions and Pooling
#This code will show us the convolutions graphically. The print (test_labels[:100]) 
#shows us the first 100 labels in the test set, and you can see that the ones at index 0, 
#index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of 
#$running the convolution on each, and you'll begin to see common features between them emerge. 
#Now, when the final dense layers are trained on this resulting data, it's working with a lot less, more targeted,
# data -- the features generated by this convolution/pooling combination.

print(val_labels[:100])

import matplotlib.pyplot as plt
def show_image(img):
  plt.figure()
  plt.imshow(val_images[img].reshape(28,28))
  plt.grid(False)
  plt.show()  

f, axarr = plt.subplots(3,2)
# By scanning the list above I saw that the 0, 23 and 28 entries are all label 9 
#donde sabemos que estas imagenes son zapatos
FIRST_IMAGE=0
SECOND_IMAGE=23
THIRD_IMAGE=28

# For shoes (0, 23, 28), Convolution_Number=1 (i.e. the second filter) shows
# the sole being filtered out very clearly

CONVOLUTION_NUMBER = 1
from tf.keras import models
layer_outputs = [layer.output for layer in model.layers]
activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)
for x in range(0,2):
  f1 = activation_model.predict(val_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[0,x].grid(False)
  f2 = activation_model.predict(val_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[1,x].grid(False)
  f3 = activation_model.predict(val_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[2,x].grid(False)


show_image(FIRST_IMAGE)
show_image(SECOND_IMAGE)
show_image(THIRD_IMAGE)
